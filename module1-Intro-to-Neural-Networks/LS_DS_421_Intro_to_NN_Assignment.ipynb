{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVfaLrjLvxvQ"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Neural Networks\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 1*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "## Define the Following:\n",
    "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
    "\n",
    "### Input Layer: \n",
    "    The very first layer of the Neural Network. This layer takes in the data that the model will be trained on and it has one node for each of the features of the data.\n",
    "### Hidden Layer:\n",
    "    These are the layers in between the input and output layers. In these layers is where the NN applies weights and biases to the input to convert to output\n",
    "### Output Layer:\n",
    "    The output layer is the final layer of the NN. This is where the NN produces its result/prediction.\n",
    "### Neuron:\n",
    "    A neuron is a node in the neural network. Like in the brain, each neuron has an activation threshhold. So when data gets passed to it and the \n",
    "    weights and biases are calculated, it passes on the information if it surpasses that threshold.\n",
    "    (This is not strictly true as it depends on the activation function. Some activation functions will pass on a corresponding weghted version of the feature,\n",
    "    some will either pass it on or not in a binary way, and some will combine the two.)\n",
    "### Weight:\n",
    "    A weight is a number that the input data gets multiplied by. This adjusts the value corresponding to its impact on the final output.\n",
    "### Activation Function:\n",
    "    This determines the threshold of where the neuron in the network fires.\n",
    "### Node Map:\n",
    "    A visual representation of a neural network that shows things like how many layers there are and how many nodes there are per layer\n",
    "### Perceptron:\n",
    "    A very simple neural network with just 1 hidden layer with 1 node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXuy9WcWzxa4"
   },
   "source": [
    "## Inputs -> Outputs\n",
    "\n",
    "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlSwIJMC0A8F"
   },
   "source": [
    "#### The network starts with information being input at the first, input, layer. It then passes to the first hidden layer. Each node in the hidden layer will multiply all the features by the given weights and add them together along with bias constant. Then the activation function will determine if the node 'fires' to the next layer or not. This process reepeats for each layer of the network, until it reaches the output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6sWR43PTwhSk"
   },
   "source": [
    "## Write your own perceptron code that can correctly classify (99.0% accuracy) a NAND gate. \n",
    "\n",
    "| x1 | x2 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = { 'x1': [0,1,0,1],\n",
    "         'x2': [0,0,1,1],\n",
    "         'y':  [1,1,1,0]\n",
    "       }\n",
    "\n",
    "df = pd.DataFrame.from_dict(data).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sgh7VFGwnXGH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    sx = sigmoid(x)\n",
    "    return sx * (1-sx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 0],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [1, 1]]), array([[1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = np.array(df[['x1', 'x2']])\n",
    "correct_outputs = np.array(df['y']).reshape(-1,1)\n",
    "\n",
    "inputs, correct_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.22481678],\n",
       "       [ 0.42217814]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = 2 * np.random.random((2,1)) -1\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ],\n",
       "       [-0.22481678],\n",
       "       [ 0.42217814],\n",
       "       [ 0.19736136]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_sum = np.dot(inputs, weights)\n",
    "\n",
    "weighted_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.5       ],\n",
       "        [0.44403134],\n",
       "        [0.60400434],\n",
       "        [0.54918081]]), array([[1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activated_output = sigmoid(weighted_sum)\n",
    "\n",
    "activated_output, correct_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5       ],\n",
       "       [ 0.55596866],\n",
       "       [ 0.39599566],\n",
       "       [-0.54918081]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = correct_outputs - activated_output\n",
    "\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.11750186],\n",
       "       [ 0.13236011],\n",
       "       [ 0.09049161],\n",
       "       [-0.12744213]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjustments = error * sigmoid_derivative(activated_output)\n",
    "\n",
    "adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.2198988 ],\n",
       "       [ 0.38522763]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights += np.dot(inputs.T, adjustments)\n",
    "\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.92272715]\n",
      " [-0.82742656]]\n",
      "[[ 0.00099449]\n",
      " [-0.00099439]]\n",
      "[[ 1.08342124e-06]\n",
      " [-1.08342112e-06]]\n",
      "[[ 1.18036876e-09]\n",
      " [-1.18036887e-09]]\n",
      "[[ 1.28594357e-12]\n",
      " [-1.28606847e-12]]\n",
      "[[ 1.38777878e-15]\n",
      " [-1.34614542e-15]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "Weights after training\n",
      "[[ 1.66533454e-16]\n",
      " [-3.19189120e-16]]\n",
      "Output after training\n",
      "[[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]]\n"
     ]
    }
   ],
   "source": [
    "weights = 2 * np.random.random((2,1)) -1\n",
    "for i in range(10000):\n",
    "    \n",
    "    weighted_sum = np.dot(inputs, weights)\n",
    "    #print(weighted_sum)\n",
    "    activated_output = sigmoid(weighted_sum)\n",
    "    #print(activated_output)\n",
    "    error = correct_outputs - activated_output\n",
    "    #print(error)\n",
    "    adjustments = error * sigmoid_derivative(activated_output)\n",
    "    #print(adjustments)\n",
    "    weights += np.dot(inputs.T, adjustments)\n",
    "    if i % 100 == 0:\n",
    "        print(weights)\n",
    "print('Weights after training')\n",
    "print(weights)\n",
    "\n",
    "print(\"Output after training\")\n",
    "print(activated_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xf7sdqVs0s4x"
   },
   "source": [
    "## Implement your own Perceptron Class and use it to classify a binary dataset: \n",
    "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
    "\n",
    "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "diabetes = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv')\n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although neural networks can handle non-normalized data, scaling or normalizing your data will improve your neural network's learning speed. Try to apply the sklearn `MinMaxScaler` or `Normalizer` to your diabetes dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "\n",
    "feats = list(diabetes)[:-1]\n",
    "\n",
    "X = diabetes[feats]\n",
    "y = diabetes['Outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      0\n",
       "2      1\n",
       "3      0\n",
       "4      1\n",
       "      ..\n",
       "763    0\n",
       "764    0\n",
       "765    0\n",
       "766    1\n",
       "767    0\n",
       "Name: Outcome, Length: 768, dtype: int64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-W0tiX1F1hh2"
   },
   "outputs": [],
   "source": [
    "##### Update this Class #####\n",
    "\n",
    "class Perceptron(object):\n",
    "    \n",
    "    def __init__(self, niter = 10):\n",
    "        self.niter = niter\n",
    "    \n",
    "    def __sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def __sigmoid_derivative(self, x):\n",
    "        sx = sigmoid(x)\n",
    "        return sx * (1-sx)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit training data\n",
    "        X : Training vectors, X.shape : [#samples, #features]\n",
    "        y : Target values, y.shape : [#samples]\n",
    "        \"\"\"\n",
    "\n",
    "        # Randomly Initialize Weights\n",
    "        weights = np.random.random((X.shape[1], 1))\n",
    "        \n",
    "        for i in range(self.niter):\n",
    "            \n",
    "            # Weighted sum of inputs / weights\n",
    "            weighted_sum = np.dot(X, weights)\n",
    "            # Activate!\n",
    "            activated_output = sigmoid(weighted_sum)\n",
    "            # Cac error\n",
    "            error = (np.array(y).reshape(-1,1)) - activated_output\n",
    "            # Update the Weights\n",
    "            adjustments = error * self._Perceptron__sigmoid_derivative(activated_output)\n",
    "            weights += np.dot(inputs.T, adjustments)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step\"\"\"\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Perceptron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sigmoid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-15b617b6ecc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-92fe0818cb51>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mactivated_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;31m# Update the Weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0madjustments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Perceptron__sigmoid_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivated_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mweights\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjustments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-92fe0818cb51>\u001b[0m in \u001b[0;36m__sigmoid_derivative\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__sigmoid_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0msx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sigmoid' is not defined"
     ]
    }
   ],
   "source": [
    "p.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QR4oAW1xdyu"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
    "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
    "- Try and implement your own backpropagation algorithm.\n",
    "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_431_Intro_to_NN_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
